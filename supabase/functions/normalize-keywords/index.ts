
import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.49.4';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const openaiApiKey = Deno.env.get('OPENAI_API_KEY');
    
    if (!openaiApiKey) {
      throw new Error('OpenAI API key not configured');
    }

    const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
    const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
    const supabase = createClient(supabaseUrl, supabaseKey);

    // R√©cup√©rer tous les mots-cl√©s existants des documents
    const { data: documents, error: fetchError } = await supabase
      .from('uploaded_documents')
      .select('id, taxonomy, ai_generated_name, original_name')
      .not('taxonomy', 'is', null);

    if (fetchError) throw fetchError;

    // Extraire tous les mots-cl√©s existants
    const allKeywords = new Set<string>();
    documents?.forEach(doc => {
      if (doc.taxonomy?.keywords) {
        doc.taxonomy.keywords.forEach((keyword: string) => {
          allKeywords.add(keyword.toLowerCase().trim());
        });
      }
    });

    console.log(`üìä Found ${allKeywords.size} unique keywords across ${documents?.length} documents`);

    // Cr√©er un dictionnaire normalis√© avec l'IA
    const keywordsList = Array.from(allKeywords);
    const normalizedDictionary = await createNormalizedDictionary(keywordsList, openaiApiKey);
    
    console.log(`üéØ Created normalized dictionary with ${Object.keys(normalizedDictionary).length} canonical terms`);

    // Retraiter tous les documents avec le nouveau dictionnaire
    let processedCount = 0;
    
    for (const document of documents || []) {
      if (document.taxonomy?.keywords) {
        const normalizedKeywords = normalizeKeywords(document.taxonomy.keywords, normalizedDictionary);
        
        // Mettre √† jour uniquement si les mots-cl√©s ont chang√©
        if (JSON.stringify(normalizedKeywords.sort()) !== JSON.stringify(document.taxonomy.keywords.sort())) {
          const updatedTaxonomy = {
            ...document.taxonomy,
            keywords: normalizedKeywords
          };

          const { error: updateError } = await supabase
            .from('uploaded_documents')
            .update({ taxonomy: updatedTaxonomy })
            .eq('id', document.id);

          if (updateError) {
            console.error(`‚ùå Error updating document ${document.id}:`, updateError);
          } else {
            processedCount++;
            console.log(`‚úÖ Updated keywords for: ${document.ai_generated_name || document.original_name}`);
          }
        }
      }
    }

    return new Response(JSON.stringify({ 
      success: true,
      processedDocuments: processedCount,
      totalDocuments: documents?.length || 0,
      dictionarySize: Object.keys(normalizedDictionary).length,
      message: `Processed ${processedCount} documents with normalized keywords`
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error) {
    console.error('‚ùå Error:', error);
    return new Response(JSON.stringify({ error: error.message }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});

async function createNormalizedDictionary(keywords: string[], openaiApiKey: string): Promise<Record<string, string>> {
  const prompt = `Tu es un expert en normalisation de mots-cl√©s pour un syst√®me de gestion documentaire m√©dical.

Analyse cette liste de mots-cl√©s et cr√©e un dictionnaire de normalisation pour √©liminer les synonymes et variantes :

${keywords.join(', ')}

R√àGLES STRICTES :
1. Groupe les synonymes sous UN SEUL terme canonique (le plus pr√©cis et professionnel)
2. √âlimine les termes trop g√©n√©riques (ex: "document", "fichier", "information")
3. Privil√©gie les termes m√©dicaux/techniques pr√©cis
4. Garde uniquement les mots-cl√©s utiles pour la recherche documentaire
5. Utilise la forme singulier et fran√ßaise

Retourne UNIQUEMENT un JSON avec cette structure :
{
  "mot_original": "terme_canonique",
  "autre_mot": "terme_canonique",
  "synonyme": "terme_canonique"
}

Exemple de normalisation :
- "chirurgie", "op√©ration", "intervention" ‚Üí "chirurgie"
- "oeil", "≈ìil", "yeux" ‚Üí "≈ìil" 
- "laser", "lasik" ‚Üí selon le contexte sp√©cifique`;

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${openaiApiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.1,
      max_tokens: 2000,
    }),
  });

  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.status}`);
  }

  const data = await response.json();
  const content = data.choices[0].message.content.trim();
  
  try {
    const jsonMatch = content.match(/\{[\s\S]*\}/);
    const jsonString = jsonMatch ? jsonMatch[0] : content;
    return JSON.parse(jsonString);
  } catch (e) {
    console.error('Failed to parse AI response:', e);
    return {};
  }
}

function normalizeKeywords(keywords: string[], dictionary: Record<string, string>): string[] {
  const normalized = keywords
    .map(keyword => {
      const lowercaseKeyword = keyword.toLowerCase().trim();
      return dictionary[lowercaseKeyword] || keyword;
    })
    .filter((keyword, index, array) => array.indexOf(keyword) === index) // Supprimer les doublons
    .filter(keyword => keyword.length > 2) // Supprimer les mots trop courts
    .sort();

  return normalized;
}
